{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re as re\n",
    "from datetime import date\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_crosstab(category, target, dataframe):\n",
    "    df = pd.crosstab(dataframe[target], dataframe[category], normalize='columns').transpose().sort_values(by=0, ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def kmeans_clusterer(category, target, dataframe, validation_dataframe, k_clusters = 0, colors = 'rainbow'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from kneed import KneeLocator\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    \n",
    "    columns= [category, target]\n",
    "    cluster_data = dataframe[columns]\n",
    "    \n",
    "    conditional_probability = category_crosstab(category, target, cluster_data)\n",
    "    cluster_data['conditional_probability'] = cluster_data[category].apply(lambda x: conditional_probability.loc[x][1])\n",
    "    \n",
    "    cluster_data['Income_count']=cluster_data['Income'].copy()\n",
    "    \n",
    "    clustering_data = cluster_data.groupby(category).agg({'Income':'sum',\n",
    "                                                          'Income_count':'count',\n",
    "                                                          'conditional_probability':'first'}).sort_values(by='Income_count')\n",
    "    clustering_data\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(clustering_data)\n",
    "\n",
    "    kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 1500,\n",
    "    \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    # A list holds the SSE values for each k\n",
    "    sse = []\n",
    "    \n",
    "    number_unique_categories = len(dataframe[category].unique())\n",
    "    if number_unique_categories > 10: \n",
    "        max_number_clusters = 11\n",
    "    else:\n",
    "        max_number_clusters = number_unique_categories\n",
    "\n",
    "    for k in range(1, max_number_clusters):\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    \n",
    "  \n",
    "    # Define Elbow point\n",
    "\n",
    "    kl = KneeLocator(range(1, max_number_clusters), \n",
    "                     sse, \n",
    "                     curve=\"convex\", \n",
    "                     direction=\"decreasing\")\n",
    "    \n",
    "    if k_clusters == 0 : \n",
    "        number_cluster = kl.elbow\n",
    "    else: \n",
    "        number_cluster = k_clusters\n",
    "    \n",
    "    # Run k means \n",
    "    \n",
    "    kmeans= KMeans(n_clusters=number_cluster, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    # assign kmeans labels to each category\n",
    "    clustering_data['kmeans_cluster'] = kmeans.labels_\n",
    "    \n",
    "    \n",
    "    # plot SSE (inertia) vs number of clusters - Improve\n",
    "    plt.style.use('dark_background')\n",
    "    plt.style.use('dark_background')\n",
    "    fig, axes = plt.subplots(3,1,figsize=(10,20))\n",
    "    fig.suptitle(category, fontsize=16)\n",
    "    \n",
    "        # first plot - inertia vs number of clusters\n",
    "        \n",
    "    x = range(1, max_number_clusters)\n",
    "    y = sse\n",
    "    #plt.style.use(\"fivethirtyeight\")\n",
    "    axes[0].plot(x, sse, marker = \"D\" )\n",
    "    plt.sca(axes[0])\n",
    "    plt.set_cmap(colors)\n",
    "    plt.title('Inertia vs Number of Clusters')\n",
    "    plt.xticks(range(1, max_number_clusters))\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.plot(x[number_cluster -1], y[number_cluster -1], 'ro')\n",
    "    \n",
    "        # second plot - number of individuals in class vs conditional probability\n",
    "    \n",
    "    sns.scatterplot(x='Income_count', \n",
    "                    y='conditional_probability', \n",
    "                    hue = 'kmeans_cluster' , \n",
    "                    data = clustering_data,\n",
    "                    palette = colors,\n",
    "                    ax = axes[1])\n",
    "    \n",
    "    plt.sca(axes[1])\n",
    "    plt.title('Number of individuals in Category vs Conditional Probability')\n",
    "    #plt.xticks(range(0, 100))\n",
    "    # Set x-axis label\n",
    "    plt.xlabel('Number of individuals in Category')\n",
    "    # Set y-axis label\n",
    "    plt.ylabel('Conditional Probability')\n",
    "    \n",
    "        # third plot - number of individuals in class that hit target vs conditional probability\n",
    "\n",
    "    sns.scatterplot(x='Income', \n",
    "                    y='conditional_probability', \n",
    "                    hue = 'kmeans_cluster' , \n",
    "                    data = clustering_data,\n",
    "                    palette = colors,\n",
    "                    ax = axes[2])\n",
    "    \n",
    "    plt.sca(axes[2])\n",
    "    plt.title('Number of individuals in Category w/ Target vs Conditional Probability')\n",
    "    #plt.xticks(range(0, 100))\n",
    "    # Set x-axis label\n",
    "    plt.xlabel('Number of individuals in Category')\n",
    "    # Set y-axis label\n",
    "    plt.ylabel('Conditional Probability')\n",
    "    \n",
    "    #3d plotting \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(category, fontsize=16)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    \n",
    "    # Data for three-dimensional scattered points\n",
    "    zdata = clustering_data['conditional_probability']\n",
    "    xdata = clustering_data['Income']\n",
    "    ydata = clustering_data['Income_count']\n",
    "    ax.scatter3D(xdata, ydata, zdata, c=clustering_data['kmeans_cluster'])#, cmap=colors);\n",
    "    ax.set_xlabel('People in Category')\n",
    "    ax.set_ylabel('People in Category w/target')\n",
    "    ax.set_zlabel('Conditional Probability');\n",
    "    \n",
    "    \n",
    "    display(clustering_data.sort_values(by='kmeans_cluster'))\n",
    "    # Add cluster as dimension \n",
    "    new_category= category +' - Clustered'\n",
    "    dataframe[new_category] = dataframe[category].apply(lambda x: clustering_data['kmeans_cluster'][x])\n",
    "    validation_dataframe[new_category] = validation_dataframe[category].apply(lambda x: clustering_data['kmeans_cluster'][x])\n",
    "    \n",
    "\n",
    "    print('Process done')\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_kmeans(clustering_data, max_k =10, k_clusters = 'else', visualize = True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from kneed import KneeLocator\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(clustering_data)\n",
    "    scaled_df = pd.DataFrame(scaled_features)\n",
    "    scaled_df.columns = clustering_data.columns\n",
    "\n",
    "    kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 1500,\n",
    "    \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    # A list holds the SSE values for each k\n",
    "    sse = []\n",
    "    \n",
    "    # Measuring SSE for different k cluster levels\n",
    "    max_number_clusters = max_k\n",
    "    \n",
    "    \n",
    "    for k in range(1, max_number_clusters):\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "        # Define Elbow point Automatically or manually. \n",
    "\n",
    "    kl = KneeLocator(range(1, max_number_clusters), \n",
    "                     sse, \n",
    "                     curve=\"convex\", \n",
    "                     direction=\"decreasing\")\n",
    "    \n",
    "    \n",
    "    if k_clusters == 'else' : \n",
    "        number_cluster = kl.elbow\n",
    "    else: \n",
    "        number_cluster = k_clusters\n",
    "    \n",
    "    if visualize == True:\n",
    "    \n",
    "        # plot SSE (inertia) vs number of clusters - Improve\n",
    "\n",
    "        plt.style.use('fivethirtyeight') # set dark style, 'cause its simply better. \n",
    "        #plt.set_cmap('Set1')\n",
    "\n",
    "        fig, axes = plt.subplots(1,1,figsize=(10,20)) #3 subplots, each with its row\n",
    "        #fig.suptitle(category, fontsize=16)\n",
    "\n",
    "            # first plot - inertia vs number of clusters\n",
    "\n",
    "        x = range(1, max_number_clusters)\n",
    "        y = sse\n",
    "        #plt.style.use(\"fivethirtyeight\")\n",
    "        axes.plot(x, sse )\n",
    "\n",
    "        plt.sca(axes) # select ax0 \n",
    "        #plt.set_cmap(colors)\n",
    "\n",
    "        plt.title('Inertia vs Number of Clusters') #title\n",
    "        plt.xticks(range(1, max_number_clusters)) # xticks\n",
    "        plt.xlabel(\"Number of Clusters\") # xlabels\n",
    "        plt.ylabel(\"SSE\") # ylabels\n",
    "        plt.plot(x[number_cluster -1], y[number_cluster -1], color='green', marker='X', \n",
    "                 linestyle='dashed', linewidth=15, markersize=25) # show that represents \n",
    "\n",
    "    \n",
    "    \n",
    "    # Run k means with right number of clusters \n",
    "    \n",
    "    kmeans= KMeans(n_clusters=number_cluster, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    # assign kmeans labels to each category\n",
    "    clustering_data['kmeans_cluster'] = kmeans.labels_\n",
    "    scaled_df['kmeans_cluster'] = kmeans.labels_\n",
    "    \n",
    "    display(clustering_data.sort_values(by='kmeans_cluster'))\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def boxplot_cluster_compparisson(dataframe):\n",
    "\n",
    "    variables = dataframe.iloc[:,:-1].columns\n",
    "    number_variables = len(variables)\n",
    "    height = number_variables*20\n",
    "\n",
    "    fig, ax = plt.subplots(number_variables,1, figsize =(height,height))\n",
    "\n",
    "    for variable, x in zip(variables, range(number_variables)): \n",
    "        sns.boxplot(x= dataframe.iloc[:,-1] ,\n",
    "                    y = dataframe[variable],\n",
    "                       ax = ax[x])     \n",
    "    return fig\n",
    "\n",
    "def kmeans_analysis(clustering_data, max_k =10, k_clusters = 'else', visualize = True):\n",
    "    \n",
    "    sc_df = simple_kmeans(clustering_data, max_k =10, k_clusters = 'else', visualize = True)\n",
    "    graphs = boxplot_cluster_compparisson(sc_df)\n",
    "    return graphs, sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_significance_PCA(pca, plotTitle):\n",
    "    \"\"\"Takes the PCA model after fit and transform, plotting the cumulative significance of each component\"\"\"\n",
    "\n",
    "    # figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # draw plots\n",
    "    ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "    ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "    ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "    # customizations\n",
    "    ax2.legend()\n",
    "    ax1.set_title(plotTitle, fontsize=14)\n",
    "    ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Eigenvalue\")\n",
    "    ax2.set_ylabel(\"Proportion\")\n",
    "    ax1.set_xlabel(\"Components\")\n",
    "    ax2.set_xlabel(\"Components\")\n",
    "    ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "    ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "    ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "    ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_analysis(subgroups_pca_dic, path_to_excel):\n",
    "   # writer = pd.ExcelWriter('/Files/test_pca.xlsx')\n",
    "\n",
    "    for subGroup in hood_subgroups_pca_dic.keys():\n",
    "        columns = hood_subgroups_dic[subGroup]\n",
    "        k = hood_subgroups_pca_dic[subGroup]\n",
    "\n",
    "        #PCA fit\n",
    "        pca = PCA(n_components=k)\n",
    "        pca_feat = pca.fit_transform(pca_data[columns])\n",
    "\n",
    "        #Creating dataframe\n",
    "        pca_feat_names = [f\"PC{i}\" for i in range(k)]\n",
    "        pca_df = pd.DataFrame(pca_feat, index=pca_data[columns].index, columns=pca_feat_names)\n",
    "\n",
    "        # Reassigning df to contain pca variables\n",
    "        pca_df = pd.concat([pca_data[columns], pca_df], axis=1)\n",
    "\n",
    "        # Interpreting each Principal Component\n",
    "        loadings = pca_df[columns + pca_feat_names].corr().loc[columns, pca_feat_names]\n",
    "        print(\"\\n\\n\\n//////////////////////////////////%s\" % subGroup)\n",
    "        display(loadings.style.applymap(_color_red_or_green))\n",
    "\n",
    "        # Returning an excel file (sorry) with the analysis\n",
    "\n",
    "        test='home_structures'\n",
    "\n",
    "        loadings.style.applymap(_color_red_or_green).to_excel(writer, subGroup)\n",
    "        #writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_row', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/donors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Analysing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Donation history\n",
    "\n",
    "- RAMNTALL  -->  Dollar amount of lifetime gifts to date\n",
    "- NGIFTALL  -->  Number of lifetime gifts to date\n",
    "- AVGGIFT  -->   Average dollar amount of gifts to date\n",
    "- NUMPROM  -->   Lifetime number of promotions received to date\n",
    "- NUMPRM12 -->   Number of promotions received in the last 12 months\n",
    "- CARDPROM -->   Lifetime number of card promotions received to date\n",
    "- CARDPRM12 -->  Number of card promotions received in the last 12 months\n",
    "- CARDGIFT -->   Number of lifetime gifts to card promotions to date\n",
    "- MINRAMNT -->   Dollar amount of smallest gift to date\n",
    "- MAXRAMNT -->   Dollar amount of largest gift to date\n",
    "\n",
    "\n",
    "### Data to generate/keep\n",
    "- AVGGIFT - Average donated amount\n",
    "- NGIFTALL / NUMPROM - Success percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_source = [\n",
    "    'RAMNTALL',\n",
    "    'NGIFTALL',\n",
    "    'AVGGIFT',\n",
    "    'NUMPROM',\n",
    "    'NUMPRM12',\n",
    "    'CARDPROM',\n",
    "    'CARDPM12',\n",
    "    'CARDGIFT', \n",
    "    'MINRAMNT', \n",
    "    'MAXRAMNT',\n",
    "    'LASTGIFT', \n",
    "    'TIMELAG', \n",
    "]\n",
    "\n",
    "filtered_df = df[columns_source].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = pd.to_datetime(df['RDATE_3'].mode(), infer_datetime_format=True)[0]\n",
    "\n",
    "current_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['first_time_delta'] = (pd.to_datetime(df['NEXTDATE'], infer_datetime_format=True) - pd.to_datetime(df['FISTDATE'], infer_datetime_format=True)).dt.days\n",
    "\n",
    "filtered_df['minmax_time_delta'] = (pd.to_datetime(df['MINRDATE'], infer_datetime_format=True) - pd.to_datetime(df['MAXRDATE'], infer_datetime_format=True)).dt.days\n",
    "\n",
    "filtered_df['maxmin_dollar_diff'] = df['MAXRAMNT'] - df['MINRAMNT']\n",
    "\n",
    "#filtered_df['customer_age'] = (pd.to_datetime([current_date for x in range(df.shape[0])]) -(pd.to_datetime(df['LASTDATE'], infer_datetime_format=True)))\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_source].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['TIMELAG'].isna()][rfa_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure datatype is correct\n",
    "df[columns_source].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix dtypes\n",
    "df['RAMNTALL'] = df['RAMNTALL'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_source].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGIFTALL / NUMPROM - Success percentage\n",
    "filtered_df['SUCCESS_PCT'] = df['NGIFTALL'] / df['NUMPROM']\n",
    "filtered_df['SUCCESS_PCT'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns\n",
    "\n",
    "- Percentage of time as each-category\n",
    "- Variance on donation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage of time as each-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_as_category(source_dataframe, target_df, category):\n",
    "    re_expression = re.compile('^RFA_\\d{1,2}$')\n",
    "\n",
    "    rfa_columns = [column for column in source_dataframe.columns.values if re_expression.match(column)]\n",
    "\n",
    "    rfas = source_dataframe[rfa_columns].copy()\n",
    "    #rint(rfas.head())\n",
    "\n",
    "    rfas = rfas.applymap(lambda val: 1 if val[0] == category else 0)\n",
    "    #rint(rfas.head())\n",
    "    \n",
    "    target_df['PCT_TIME_LAPSED_%s' % category] =  rfas.sum(axis=1) / df['NUMPROM']\n",
    "\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = get_percentage_as_category(df, filtered_df, 'F')\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = get_percentage_as_category(df, filtered_df, 'N')\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df= get_percentage_as_category(df, filtered_df, 'A')\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df= get_percentage_as_category(df, filtered_df, 'L')\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df= get_percentage_as_category(df, filtered_df, 'I')\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df= get_percentage_as_category(df, filtered_df, 'S')\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def rfa_normalizer(source_dataframe, target_df, byte, category):\n",
    "    re_expression = re.compile('^RFA_\\d{1,2}$')\n",
    "\n",
    "    rfa_columns = [column for column in source_dataframe.columns.values if re_expression.match(column)]\n",
    "\n",
    "    rfas = source_dataframe[rfa_columns].copy()\n",
    "    print(rfas.head())\n",
    "\n",
    "    rfas = rfas.applymap(lambda val: 1 if val[byte] == category else 0)\n",
    "    print(rfas.head())\n",
    "    \n",
    "    target_df['PCT_TIME_LAPSED_%s' % category] =  rfas.sum(axis=1) / df['NUMPROM']\n",
    "\n",
    "    return target_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "category1 = { 'S': 5, \n",
    "             'A' : 4,\n",
    "             'N' : 3, \n",
    "             'F' : 2, \n",
    "             'L' : 1, \n",
    "             'I' : 0\n",
    "                    }\n",
    "\n",
    "category2 = {'4': 4, \n",
    "             '3': 3,\n",
    "             '2' : 2,\n",
    "             '1': 1\n",
    "                    }\n",
    "\n",
    "category3 = {}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_expression = re.compile('^RFA_\\d{1,2}$')\n",
    "rfa_columns = [column for column in df.columns.values if re_expression.match(column)]\n",
    "df[df['RFA_22'].str.startswith('P')][rfa_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_expression = re.compile('^RAMNT_\\d{1,2}$')\n",
    "ramnt_columns = [column for column in df.columns.values if re_expression.match(column)]\n",
    "df[df['RFA_22'].str.startswith('P')][ramnt_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance on donation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_expression = re.compile('^RAMNT_\\d{1,2}$')\n",
    "\n",
    "ramt_columns = [column for column in df.columns.values if re_expression.match(column)]\n",
    "ramt_columns[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramts = df[ramt_columns].copy()\n",
    "ramts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['GIFT_VAR'] = ramts.var(axis=1)\n",
    "filtered_df['GIFT_VAR'].fillna(0, inplace=True)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation check on filtered columns so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = filtered_df.corr()\n",
    "corr = corr[np.abs(corr) > 0.45]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Interests columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_interests= [\n",
    "    'COLLECT1',\n",
    "    'VETERANS',\n",
    "    'BIBLE',\n",
    "    'CATLG',\n",
    "    'HOMEE',\n",
    "    'PETS',\n",
    "    'CDPLAY',\n",
    "    'STEREO',\n",
    "    'PCOWNERS',\n",
    "    'PHOTO',\n",
    "    'CRAFTS',\n",
    "    'FISHER',\n",
    "    'GARDENIN',\n",
    "    'BOATS',\n",
    "    'WALKER',\n",
    "    'KIDSTUFF',\n",
    "    'CARDS',\n",
    "    'PLATES'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests_df = df[columns_interests].copy()\n",
    "interests_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests_df = interests_df.applymap(lambda val: 1 if val == 'Y' else 0)\n",
    "interests_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = interests_df.corr()\n",
    "corr = corr[np.abs(corr) > 0.45]\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    low correlations, going to ignore for now since there's to many variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Country Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['county_size'] = df['GEOCODE2'].replace({'A':1, 'B':2, 'C':3, 'D':4, ' ': np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocode_df = df[['GEOCODE','GEOCODE2']].copy()\n",
    "geocode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing wealth columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WEALTH1'].count(), df['WEALTH1'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WEALTH2'].count(), df['WEALTH2'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.WEALTH1.max(), df.WEALTH1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.WEALTH2.max(), df.WEALTH2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_df = df[['WEALTH1','WEALTH2']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_non_NaN = wealth_df.mean(axis=1).count() / wealth_df.shape[0]\n",
    "print('Percentage of columns with value after merge: %1.2f%%' % (pct_non_NaN *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_df['Merged'] = wealth_df.max(axis=1)\n",
    "wealth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['WEALTH'] = wealth_df.Merged\n",
    "filtered_df.dropna(subset=['WEALTH'], inplace=True)\n",
    "del wealth_df\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df = df[['GENDER','INCOME','HOMEOWNR']].copy()\n",
    "other_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.HOMEOWNR.replace(['',' '], 'U', inplace=True)\n",
    "other_df.HOMEOWNR.replace(['U'], 0, inplace=True)\n",
    "other_df.HOMEOWNR.replace(['H'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.GENDER.replace(['A','C'],'J', inplace=True)\n",
    "other_df.GENDER.replace(' ','U', inplace=True)\n",
    "gender_dummies = pd.get_dummies(other_df.GENDER, prefix='GENDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df.drop(columns='GENDER', inplace=True)\n",
    "other_df = other_df.join(gender_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.concat([filtered_df, other_df], axis=1)\n",
    "filtered_df.dropna(subset=['INCOME'], inplace=True)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Children columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_columns = [\n",
    "    'CHILD03',\n",
    "    'CHILD07',\n",
    "    'CHILD12',\n",
    "    'CHILD18',\n",
    "    'NUMCHLD'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_df = df[children_columns].copy()\n",
    "children_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_df.replace(' ', 0, inplace=True)\n",
    "children_df.replace('', 0, inplace=True)\n",
    "children_df.replace(['M','F','B'], 1, inplace=True)\n",
    "children_df.fillna(0, inplace=True)\n",
    "\n",
    "children_df = children_df.astype('int64')\n",
    "children_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_df['SUM_ageGap_columns'] = children_df[children_columns[:-1]].sum(axis=1)\n",
    "children_df.drop(columns=children_columns[:-1], inplace=True)\n",
    "\n",
    "children_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['CHILDREN'] = children_df.max(axis=1)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Neighborhood Socio Economic Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(df['DOMAIN'].mode()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_economic_status = df['DOMAIN'].apply(lambda x : int(x[1]) if x != ' ' else int(df['DOMAIN'].mode()[0][1])) # assign mode in case of NaN\n",
    "#socio_economic_status = pd.get_dummies(socio_economic_status, prefix='SES_')\n",
    "\n",
    "socio_economic_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rurality = df['DOMAIN'].apply(lambda x : x[0] if x != ' ' else df['DOMAIN'].mode()[0][0])\n",
    "rurality.replace( {'U':4,\n",
    "                  'C':3,\n",
    "                  'S':2, \n",
    "                  'T':1,\n",
    "                  'R':0}, inplace=True)\n",
    "rurality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['ses'] = socio_economic_status\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['rurality'] = rurality\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding some more useful columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MDMAUD_R'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MDMAUD_F'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MDMAUD_A'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OSOURCE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NOEXCH'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['RECINHSE'] = df['RECINHSE'].apply(lambda x : 1 if x == 'X' else 0)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['RECP3'] = df['RECP3'].apply(lambda x : 1 if x == 'X' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['RECPGVG'] = df['RECPGVG'].apply(lambda x : 1 if x == 'X' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['RECSWEEP'] = df['RECSWEEP'].apply(lambda x : 1 if x == 'X' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['HIT'] = df['HIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['MAJOR'] = df['MAJOR'].apply(lambda x : 1 if x == 'X' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['GEOCODE','GEOCODE2']].isna()\n",
    "\n",
    "df[((df['GEOCODE'].isna() == False) | (df['GEOCODE2'].isna() == False))]\n",
    "\n",
    "# Later on, we can reduce number of NaN by correcting this geocode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/164917/should-data-be-centeredscaled-before-applying-t-sne \n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_filtered_df = pd.DataFrame(scaler.fit_transform(filtered_df), columns = filtered_df.columns)\n",
    "scaled_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = filtered_df.corr()\n",
    "corr = corr[corr > 0.4]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(corr, annot=True, linewidths=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    overall looking ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "k_range = range(1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    k_means = KMeans(n_clusters=k)\n",
    "    k_means.fit(filtered_df)\n",
    "    inertia.append(k_means.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(k_range, inertia, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = scaled_filtered_df\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_embedded[:,0]\n",
    "y =X_embedded[:,1]\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.scatterplot(x,y, hue = filtered_df['GENDER_F'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = len(filtered_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in filtered_df.columns: \n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.scatterplot(x,y, hue = filtered_df[column])\n",
    "    plt.savefig('Images/%s_TSNE.png' % column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = scaled_filtered_df\n",
    "\n",
    "X_embedded3 = TSNE(n_components=3).fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(X_embedded3, columns = ['0','1','2'])\n",
    "df1['MAJOR'] = filtered_df['MAJOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "#df = px.data.iris()\n",
    "fig = px.scatter_3d(df1, x='0', y='1', z='2',\n",
    "              color='MAJOR')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "k_means = KMeans(n_clusters=3)\n",
    "k_means.fit(filtered_df)\n",
    "\n",
    "new_column = 'cluster'\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "\n",
    "filtered_df[new_column] = k_means.predict(selector.fit_transform(filtered_df))\n",
    "\n",
    "\n",
    "selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.groupby(new_column).mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctimes = ['PCT_TIME_LAPSED_F', 'PCT_TIME_LAPSED_N', 'PCT_TIME_LAPSED_L', 'PCT_TIME_LAPSED_S', 'PCT_TIME_LAPSED_A', 'PCT_TIME_LAPSED_I']\n",
    "target =  ['AVGGIFT', 'SUCCESS_PCT']\n",
    "total = pctimes+target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "new_column = 'cluster_affinity'\n",
    "\n",
    "#selector = VarianceThreshold()\n",
    "\n",
    "filtered_df[new_column] = model.fit_predict(filtered_df[total])\n",
    "\n",
    "\n",
    "#filtered_df[new_column] = model.fit_predict(selector.fit_transform(filtered_df.iloc[:,:-1]))\n",
    "\n",
    "\n",
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['cluster_affinity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.Series(model.labels_).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = boxplot_cluster_compparisson(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure.savefig('Images/pretty_boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
